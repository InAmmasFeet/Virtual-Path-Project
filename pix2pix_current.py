# -*- coding: utf-8 -*-
"""Pix2Pix_current.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNvxvOSpv2Bk58rb-n88YwkRXkqvttzq
"""

import os
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import cv2
import time
from tqdm import tqdm
from PIL import Image, ImageFile
import tifffile
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

Image.MAX_IMAGE_PIXELS = None
ImageFile.LOAD_TRUNCATED_IMAGES = True

print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.config.list_physical_devices('GPU'))

class BalancedConfig:
    DATA_DIR = "/kaggle/working/"
    IF_TARGET_FILENAME = "CD8_CD163_PD1_AF_scaled_corrected.tif"
    OUTPUT_DIR = "./balanced_output"
    CHECKPOINT_DIR = "./balanced_checkpoints"
    SAMPLE_DIR = "./balanced_samples"

    EPOCHS = 120
    BATCH_SIZE = 2
    IMG_HEIGHT = 256
    IMG_WIDTH = 256
    PATCH_SIZE = 256
    STRIDE = 128

    LAMBDA_L1 = 15.0
    LAMBDA_SSIM = 30.0
    LAMBDA_GRADIENT = 10.0
    LAMBDA_CHANNEL_CONSISTENCY = 15.0
    LAMBDA_FEATURE_MATCH = 8.0
    LAMBDA_GAN = 3.0

    LAMBDA_BRIGHT_WEIGHTED = 5.0
    LAMBDA_BRIGHT_FOCAL = 3.0

    LEARNING_RATE = 2e-4
    DISC_LEARNING_RATE = 2e-4
    BETA1 = 0.5
    BETA2 = 0.999

    TARGET_SSIM = 0.75
    TARGET_SPOT_ACC = 0.3

    PHASE1_EPOCHS = 30
    PHASE2_EPOCHS = 60
    PHASE3_EPOCHS = 90

    CHECKPOINT_INTERVAL = 10
    SAMPLE_INTERVAL = 3
    BACKGROUND_THRESHOLD = 21.0

config = BalancedConfig()

for directory in [config.OUTPUT_DIR, config.CHECKPOINT_DIR, config.SAMPLE_DIR]:
    os.makedirs(directory, exist_ok=True)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"{len(gpus)} Physical GPUs configured")
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")

class StructureAttention(layers.Layer):
    def __init__(self, filters, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters

    def build(self, input_shape):
        self.query = layers.Conv2D(self.filters // 8, 1, padding='same')
        self.key = layers.Conv2D(self.filters // 8, 1, padding='same')
        self.value = layers.Conv2D(self.filters, 1, padding='same')
        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros')

    def call(self, x):
        batch, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]

        query = tf.reshape(self.query(x), [batch, h * w, -1])
        key = tf.reshape(self.key(x), [batch, h * w, -1])
        value = tf.reshape(self.value(x), [batch, h * w, c])

        attention = tf.nn.softmax(tf.matmul(query, key, transpose_b=True))

        out = tf.matmul(attention, value)
        out = tf.reshape(out, [batch, h, w, c])

        return x + self.gamma * out

class BrightSpotEnhancer(layers.Layer):
    def __init__(self, filters, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters

    def build(self, input_shape):
        self.conv1 = layers.Conv2D(self.filters // 2, 1, padding='same', activation='relu')
        self.conv3 = layers.Conv2D(self.filters // 2, 3, padding='same', activation='relu')
        self.conv5 = layers.Conv2D(self.filters // 2, 5, padding='same', activation='relu')

        self.fusion = layers.Conv2D(self.filters, 1, padding='same')

        self.gate = layers.Conv2D(1, 1, padding='same', activation='sigmoid')

    def call(self, x):
        f1 = self.conv1(x)
        f3 = self.conv3(x)
        f5 = self.conv5(x)

        concat = layers.Concatenate()([f1, f3, f5])
        fused = self.fusion(concat)

        gate = self.gate(fused)

        return x * (1.0 + gate * 1.5)

def ssim_loss(y_true, y_pred):
    y_true_01 = (y_true + 1.0) / 2.0
    y_pred_01 = (y_pred + 1.0) / 2.0

    ssim_values = []
    for c in range(3):
        ssim_ch = tf.image.ssim(y_true_01[:,:,:,c:c+1], y_pred_01[:,:,:,c:c+1], max_val=1.0)
        ssim_values.append(tf.reduce_mean(ssim_ch))

    return 1.0 - tf.reduce_mean(ssim_values)

def gradient_loss(y_true, y_pred):
    def get_gradients(img):
        dx = img[:, :, 1:, :] - img[:, :, :-1, :]
        dy = img[:, 1:, :, :] - img[:, :-1, :, :]
        dx = tf.pad(dx, [[0,0], [0,0], [0,1], [0,0]])
        dy = tf.pad(dy, [[0,0], [0,1], [0,0], [0,0]])
        return tf.sqrt(dx**2 + dy**2 + 1e-8)

    return tf.reduce_mean(tf.abs(get_gradients(y_true) - get_gradients(y_pred)))

def channel_consistency_loss(y_true, y_pred):
    losses = []
    for c in range(3):
        losses.append(tf.reduce_mean(tf.abs(y_true[:,:,:,c] - y_pred[:,:,:,c])))
    return tf.reduce_mean(losses)

def bright_weighted_loss(y_true, y_pred):
    y_true_01 = (y_true + 1.0) / 2.0
    y_pred_01 = (y_pred + 1.0) / 2.0

    brightness = tf.reduce_max(y_true_01, axis=-1, keepdims=True)

    weight = 1.0 + tf.nn.tanh(brightness * 3.0) * 3.0

    diff = tf.abs(y_true_01 - y_pred_01)
    weighted_diff = diff * weight

    return tf.reduce_mean(weighted_diff)

def bright_focal_loss(y_true, y_pred):
    y_true_01 = (y_true + 1.0) / 2.0
    y_pred_01 = (y_pred + 1.0) / 2.0

    losses = []
    for c in range(3):
        true_ch = y_true_01[:, :, :, c]
        pred_ch = y_pred_01[:, :, :, c]

        flat = tf.reshape(true_ch, [tf.shape(true_ch)[0], -1])
        k = tf.maximum(1, tf.cast(tf.cast(tf.shape(flat)[1], tf.float32) * 0.15, tf.int32))
        threshold = tf.map_fn(
            lambda x: tf.nn.top_k(x, k=k)[0][-1],
            flat,
            fn_output_signature=tf.float32
        )
        threshold = tf.reshape(threshold, [tf.shape(true_ch)[0], 1, 1])

        bright_mask = tf.nn.sigmoid((true_ch - threshold) * 10.0)

        diff = tf.abs(true_ch - pred_ch)
        focal_weight = bright_mask * (1.0 + diff)

        losses.append(tf.reduce_mean(diff * focal_weight))

    return tf.reduce_mean(losses)

def calculate_spot_accuracy(y_true, y_pred):
    y_true_01 = (y_true + 1.0) / 2.0
    y_pred_01 = (y_pred + 1.0) / 2.0

    ious = []
    for c in range(3):
        true_ch = y_true_01[:, :, :, c]
        pred_ch = y_pred_01[:, :, :, c]

        flat = tf.reshape(true_ch, [tf.shape(true_ch)[0], -1])
        k = tf.maximum(1, tf.cast(tf.cast(tf.shape(flat)[1], tf.float32) * 0.15, tf.int32))
        threshold = tf.map_fn(
            lambda x: tf.nn.top_k(x, k=k)[0][-1],
            flat,
            fn_output_signature=tf.float32
        )
        threshold = tf.reshape(threshold, [tf.shape(true_ch)[0], 1, 1])

        true_bright = tf.cast(true_ch > threshold, tf.float32)
        pred_bright = tf.cast(pred_ch > threshold, tf.float32)

        intersection = tf.reduce_sum(true_bright * pred_bright)
        union = tf.reduce_sum(true_bright) + tf.reduce_sum(pred_bright) - intersection

        ious.append(intersection / (union + 1e-7))

    return tf.reduce_mean(ious)

def load_tiff_image_correctly(path):
    try:
        with tifffile.TiffFile(path) as tif:
            image_data = tif.series[0].asarray()
        if image_data.ndim == 3 and image_data.shape[0] < image_data.shape[1]:
            image_data = image_data.transpose((1, 2, 0))
        if image_data.shape[-1] > 3:
            image_data = image_data[:, :, :3]
        return image_data
    except Exception as e:
        print(f"Error loading {os.path.basename(path)}: {e}")
        return None

def load_image_pair(pair_dir):
    source_path = os.path.join(pair_dir, "warped_source.tiff")
    target_path = os.path.join(pair_dir, config.IF_TARGET_FILENAME)
    if not os.path.exists(source_path) or not os.path.exists(target_path):
        return None, None
    try:
        source = np.array(Image.open(source_path).convert("RGB"))
    except Exception as e:
        return None, None
    target = load_tiff_image_correctly(target_path)
    if source is None or target is None:
        return None, None
    if target.ndim != 3 or target.shape[2] != 3:
        return None, None
    if source.shape[:2] != target.shape[:2]:
        target = cv2.resize(target, (source.shape[1], source.shape[0]), interpolation=cv2.INTER_NEAREST)
    return source, target

def create_aligned_patches(source, target, patch_size, stride, background_threshold):
    h, w = source.shape[:2]
    source_patches, target_patches = [], []
    for y in range(0, h - patch_size + 1, stride):
        for x in range(0, w - patch_size + 1, stride):
            source_patch = source[y:y+patch_size, x:x+patch_size]
            target_patch = target[y:y+patch_size, x:x+patch_size]
            if np.mean(target_patch) > background_threshold:
                source_patches.append(source_patch)
                target_patches.append(target_patch)
    return source_patches, target_patches

def load_all_data():
    print("Loading data...")
    all_source_patches, all_target_patches = [], []
    pair_dirs = [os.path.join(config.DATA_DIR, f"Pair{i}") for i in range(1, 21)]
    for pair_dir in tqdm(pair_dirs, desc="Loading pairs"):
        if not os.path.exists(pair_dir):
            continue
        source, target = load_image_pair(pair_dir)
        if source is not None and target is not None:
            source_p, target_p = create_aligned_patches(
                source, target, config.PATCH_SIZE, config.STRIDE,
                background_threshold=config.BACKGROUND_THRESHOLD
            )
            all_source_patches.extend(source_p)
            all_target_patches.extend(target_p)
    print(f"Total patches loaded: {len(all_source_patches)}")
    return np.array(all_source_patches), np.array(all_target_patches)

MAX_LOG_VALUE = np.log1p(65535)

def normalize_source(image):
    return (tf.cast(image, tf.float32) / 127.5) - 1.0

def normalize_target(image):
    image = tf.cast(image, tf.float32)
    image = tf.math.log1p(image)
    return (image / (MAX_LOG_VALUE / 2.0)) - 1.0

def augmentation(source, target):
    if tf.random.uniform(()) > 0.5:
        source, target = tf.image.flip_left_right(source), tf.image.flip_left_right(target)
    if tf.random.uniform(()) > 0.5:
        source, target = tf.image.flip_up_down(source), tf.image.flip_up_down(target)

    k = tf.random.uniform([], 0, 4, dtype=tf.int32)
    source, target = tf.image.rot90(source, k), tf.image.rot90(target, k)

    return source, target

def preprocess_train(source, target):
    source = normalize_source(source)
    target = normalize_target(target)
    return augmentation(source, target)

def preprocess_val(source, target):
    return normalize_source(source), normalize_target(target)

def conv_block(x, filters, size=3, strides=1, use_structure_attention=False, use_bright_enhancer=False):
    x = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    if use_structure_attention:
        x = StructureAttention(filters)(x)
    if use_bright_enhancer:
        x = BrightSpotEnhancer(filters)(x)

    return x

def residual_block(x, filters):
    shortcut = x
    x = conv_block(x, filters, 3)
    x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)
    return x

def balanced_generator():
    inputs = layers.Input(shape=[config.IMG_HEIGHT, config.IMG_WIDTH, 3])

    e1 = conv_block(inputs, 64, 7)
    e1 = conv_block(e1, 64, 3)

    e2 = conv_block(e1, 128, 3, strides=2)
    e2 = conv_block(e2, 128, 3, use_structure_attention=True)

    e3 = conv_block(e2, 256, 3, strides=2)
    e3 = conv_block(e3, 256, 3, use_structure_attention=True)

    e4 = conv_block(e3, 512, 3, strides=2)
    e4 = conv_block(e4, 512, 3)

    bottleneck = e4
    for i in range(6):
        bottleneck = residual_block(bottleneck, 512)

    bottleneck = StructureAttention(512)(bottleneck)

    d3 = layers.Conv2DTranspose(256, 3, strides=2, padding='same')(bottleneck)
    d3 = layers.BatchNormalization()(d3)
    d3 = layers.ReLU()(d3)
    d3 = layers.Concatenate()([d3, e3])
    d3 = conv_block(d3, 256, 3, use_bright_enhancer=True)

    d2 = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(d3)
    d2 = layers.BatchNormalization()(d2)
    d2 = layers.ReLU()(d2)
    d2 = layers.Concatenate()([d2, e2])
    d2 = conv_block(d2, 128, 3, use_bright_enhancer=True)

    d1 = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(d2)
    d1 = layers.BatchNormalization()(d1)
    d1 = layers.ReLU()(d1)
    d1 = layers.Concatenate()([d1, e1])
    d1 = conv_block(d1, 64, 3)

    shared = conv_block(d1, 48, 3)

    cd8_branch = conv_block(shared, 24, 3, use_bright_enhancer=True)
    cd8_out = layers.Conv2D(1, 3, padding='same', activation='tanh')(cd8_branch)

    cd163_branch = conv_block(shared, 24, 3, use_bright_enhancer=True)
    cd163_out = layers.Conv2D(1, 3, padding='same', activation='tanh')(cd163_branch)

    pd1_branch = conv_block(shared, 24, 3, use_bright_enhancer=True)
    pd1_out = layers.Conv2D(1, 3, padding='same', activation='tanh')(pd1_branch)

    concat_features = layers.Concatenate()([cd8_branch, cd163_branch, pd1_branch])

    combination_weights = layers.Conv2D(3, 1, padding='same', activation='softmax')(concat_features)

    cd8_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1))(combination_weights)
    cd163_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,1], -1))(combination_weights)
    pd1_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,2], -1))(combination_weights)

    cd8_weighted = layers.Multiply()([cd8_out, cd8_weight])
    cd163_weighted = layers.Multiply()([cd163_out, cd163_weight])
    pd1_weighted = layers.Multiply()([pd1_out, pd1_weight])

    outputs = layers.Concatenate()([cd8_weighted, cd163_weighted, pd1_weighted])

    outputs = conv_block(outputs, 32, 3)
    outputs = layers.Conv2D(3, 1, padding='same', activation='tanh')(outputs)

    return keras.Model(inputs=inputs, outputs=outputs, name='balanced_generator')

def balanced_discriminator():
    input_img = layers.Input(shape=[None, None, 3])
    target_img = layers.Input(shape=[None, None, 3])

    x = layers.Concatenate()([input_img, target_img])

    features = []

    f1 = layers.Conv2D(64, 4, strides=2, padding='same')(x)
    f1 = layers.LeakyReLU(0.2)(f1)
    features.append(f1)

    f2 = layers.Conv2D(128, 4, strides=2, padding='same')(f1)
    f2 = layers.BatchNormalization()(f2)
    f2 = layers.LeakyReLU(0.2)(f2)
    features.append(f2)

    f3 = layers.Conv2D(256, 4, strides=2, padding='same')(f2)
    f3 = layers.BatchNormalization()(f3)
    f3 = layers.LeakyReLU(0.2)(f3)
    features.append(f3)

    f4 = layers.Conv2D(512, 4, strides=1, padding='same')(f3)
    f4 = layers.BatchNormalization()(f4)
    f4 = layers.LeakyReLU(0.2)(f4)
    features.append(f4)

    output = layers.Conv2D(1, 4, padding='same')(f4)

    return keras.Model(inputs=[input_img, target_img],
                      outputs=[output] + features,
                      name='balanced_discriminator')

def discriminator_loss(real_outputs, fake_outputs):
    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
        tf.ones_like(real_outputs[0]), real_outputs[0]))
    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
        tf.zeros_like(fake_outputs[0]), fake_outputs[0]))
    return (real_loss + fake_loss) * 0.5

def generator_gan_loss(fake_outputs):
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
        tf.ones_like(fake_outputs[0]), fake_outputs[0]))

def feature_matching_loss(real_features, fake_features):
    loss = 0
    for real_feat, fake_feat in zip(real_features[1:], fake_features[1:]):
        loss += tf.reduce_mean(tf.abs(real_feat - fake_feat))
    return loss / len(real_features[1:])

def get_phase_weights(epoch):

    if epoch <= config.PHASE1_EPOCHS:
        return {
            'l1': config.LAMBDA_L1,
            'ssim': config.LAMBDA_SSIM,
            'gradient': config.LAMBDA_GRADIENT,
            'channel': config.LAMBDA_CHANNEL_CONSISTENCY,
            'feature': config.LAMBDA_FEATURE_MATCH,
            'gan': config.LAMBDA_GAN,
            'bright_weighted': 0.0,
            'bright_focal': 0.0
        }

    elif epoch <= config.PHASE2_EPOCHS:
        progress = (epoch - config.PHASE1_EPOCHS) / (config.PHASE2_EPOCHS - config.PHASE1_EPOCHS)
        return {
            'l1': config.LAMBDA_L1,
            'ssim': config.LAMBDA_SSIM,
            'gradient': config.LAMBDA_GRADIENT,
            'channel': config.LAMBDA_CHANNEL_CONSISTENCY,
            'feature': config.LAMBDA_FEATURE_MATCH,
            'gan': config.LAMBDA_GAN,
            'bright_weighted': config.LAMBDA_BRIGHT_WEIGHTED * progress,
            'bright_focal': config.LAMBDA_BRIGHT_FOCAL * progress
        }

    else:
        return {
            'l1': config.LAMBDA_L1 * 0.8,
            'ssim': config.LAMBDA_SSIM * 0.9,
            'gradient': config.LAMBDA_GRADIENT,
            'channel': config.LAMBDA_CHANNEL_CONSISTENCY,
            'feature': config.LAMBDA_FEATURE_MATCH,
            'gan': config.LAMBDA_GAN,
            'bright_weighted': config.LAMBDA_BRIGHT_WEIGHTED * 1.5,
            'bright_focal': config.LAMBDA_BRIGHT_FOCAL * 1.5
        }

@tf.function
def train_step(generator, discriminator, gen_optimizer, disc_optimizer,
               input_image, real_image, epoch):

    with tf.GradientTape(persistent=True) as tape:
        fake_image = generator(input_image, training=True)

        real_image = tf.cast(real_image, tf.float32)
        fake_image = tf.cast(fake_image, tf.float32)

        disc_real = discriminator([input_image, real_image], training=True)
        disc_fake = discriminator([input_image, fake_image], training=True)

        weights = get_phase_weights(epoch)

        disc_loss = discriminator_loss(disc_real, disc_fake)

        l1_loss = tf.reduce_mean(tf.abs(real_image - fake_image)) * weights['l1']
        ssim_loss_val = ssim_loss(real_image, fake_image) * weights['ssim']
        grad_loss = gradient_loss(real_image, fake_image) * weights['gradient']
        channel_loss = channel_consistency_loss(real_image, fake_image) * weights['channel']

        bright_weighted = bright_weighted_loss(real_image, fake_image) * weights['bright_weighted']
        bright_focal = bright_focal_loss(real_image, fake_image) * weights['bright_focal']

        feat_loss = feature_matching_loss(disc_real, disc_fake) * weights['feature']
        gan_loss = generator_gan_loss(disc_fake) * weights['gan']

        total_gen_loss = (l1_loss + ssim_loss_val + grad_loss + channel_loss +
                         bright_weighted + bright_focal + feat_loss + gan_loss)

    gen_grads = tape.gradient(total_gen_loss, generator.trainable_variables)
    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 1.0)
    gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))

    if tf.reduce_mean(disc_loss) > 0.1:
        disc_grads = tape.gradient(disc_loss, discriminator.trainable_variables)
        disc_grads, _ = tf.clip_by_global_norm(disc_grads, 1.0)
        disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))

    del tape

    current_ssim = tf.image.ssim((real_image + 1.0) / 2.0, (fake_image + 1.0) / 2.0, max_val=1.0)
    spot_acc = calculate_spot_accuracy(real_image, fake_image)

    return {
        'gen_loss': total_gen_loss,
        'disc_loss': disc_loss,
        'ssim': tf.reduce_mean(current_ssim),
        'spot_accuracy': spot_acc,
        'l1': l1_loss / (weights['l1'] + 1e-8),
        'bright_weighted': bright_weighted / (weights['bright_weighted'] + 1e-8) if weights['bright_weighted'] > 0 else 0,
        'bright_focal': bright_focal / (weights['bright_focal'] + 1e-8) if weights['bright_focal'] > 0 else 0
    }

def generate_samples(model, test_inputs, test_targets, epoch, save_dir):
    if test_inputs is None:
        return None, None

    num_samples = min(3, test_inputs.shape[0])
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))
    if num_samples == 1:
        axes = axes.reshape(1, -1)

    ssim_scores = []
    spot_accs = []

    for i in range(num_samples):
        input_image = tf.expand_dims(test_inputs[i], 0)
        target_image = tf.expand_dims(test_targets[i], 0)
        prediction = model(input_image, training=False)[0]

        source_img = (input_image[0] * 0.5 + 0.5).numpy()

        target_log = (target_image[0] + 1.0) * (MAX_LOG_VALUE / 2.0)
        pred_log = (prediction + 1.0) * (MAX_LOG_VALUE / 2.0)

        target_linear = tf.math.expm1(target_log).numpy()
        pred_linear = tf.math.expm1(pred_log).numpy()

        target_enhanced = np.zeros_like(target_linear)
        pred_enhanced = np.zeros_like(pred_linear)

        for c in range(3):
            tg_chan = target_linear[:, :, c]
            if tg_chan.max() > tg_chan.min():
                p1, p99 = np.percentile(tg_chan, (1, 99))
                target_enhanced[:, :, c] = np.clip((tg_chan - p1) / (p99 - p1 + 1e-9), 0, 1)

            pr_chan = pred_linear[:, :, c]
            if pr_chan.max() > pr_chan.min():
                p1, p99 = np.percentile(pr_chan, (1, 99))
                pred_enhanced[:, :, c] = np.clip((pr_chan - p1) / (p99 - p1 + 1e-9), 0, 1)

        axes[i, 0].imshow(source_img)
        axes[i, 0].set_title("Input (H&E)")
        axes[i, 0].axis('off')

        axes[i, 1].imshow(target_enhanced)
        axes[i, 1].set_title("Ground Truth (IF)")
        axes[i, 1].axis('off')

        target_f32 = tf.cast(target_image[0], tf.float32)
        pred_f32 = tf.cast(prediction, tf.float32)
        ssim = tf.image.ssim((target_f32 + 1.0) / 2.0, (pred_f32 + 1.0) / 2.0, max_val=1.0).numpy()
        spot_acc = calculate_spot_accuracy(tf.expand_dims(target_f32, 0), tf.expand_dims(pred_f32, 0)).numpy()

        ssim_scores.append(ssim)
        spot_accs.append(spot_acc)

        axes[i, 2].imshow(pred_enhanced)
        axes[i, 2].set_title(f"Predicted\nSSIM: {ssim:.3f} | Spot: {spot_acc:.3f}")
        axes[i, 2].axis('off')

    avg_ssim = np.mean(ssim_scores)
    avg_spot = np.mean(spot_accs)

    fig.suptitle(f'Epoch {epoch} - SSIM: {avg_ssim:.3f}, Spot Acc: {avg_spot:.3f}', fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:04d}.png'), dpi=150, bbox_inches='tight')
    plt.close()

    return avg_ssim, avg_spot

def train_balanced_model():
    print("=" * 80)
    print("BALANCED TRAINING: STRUCTURE + BRIGHT SPOTS")
    print("Phase 1 (1-30): Structure learning only")
    print("Phase 2 (31-60): Gradual bright spot introduction")
    print("Phase 3 (61-120): Refinement with full losses")
    print("=" * 80)

    source_patches, target_patches = load_all_data()

    train_source, val_source, train_target, val_target = train_test_split(
        source_patches, target_patches, test_size=0.1, random_state=42
    )
    print(f"Train: {len(train_source)}, Val: {len(val_source)}")

    AUTOTUNE = tf.data.AUTOTUNE
    train_dataset = tf.data.Dataset.from_tensor_slices((train_source, train_target))
    train_dataset = train_dataset.map(preprocess_train, num_parallel_calls=AUTOTUNE)
    train_dataset = train_dataset.shuffle(1000).batch(config.BATCH_SIZE).prefetch(AUTOTUNE)

    val_dataset = tf.data.Dataset.from_tensor_slices((val_source, val_target))
    val_dataset = val_dataset.map(preprocess_val, num_parallel_calls=AUTOTUNE)
    val_dataset = val_dataset.batch(config.BATCH_SIZE).prefetch(AUTOTUNE)

    val_sample_indices = np.random.choice(len(val_source), min(3, len(val_source)), replace=False)
    val_sample_source = val_source[val_sample_indices]
    val_sample_target = val_target[val_sample_indices]

    test_input = tf.stack([preprocess_val(s, t)[0] for s, t in zip(val_sample_source, val_sample_target)])
    test_target = tf.stack([preprocess_val(s, t)[1] for s, t in zip(val_sample_source, val_sample_target)])

    print("\nBuilding models...")
    generator = balanced_generator()
    discriminator = balanced_discriminator()

    print(f"Generator params: {generator.count_params():,}")
    print(f"Discriminator params: {discriminator.count_params():,}")

    gen_optimizer = tf.optimizers.Adam(config.LEARNING_RATE, beta_1=config.BETA1)
    disc_optimizer = tf.optimizers.Adam(config.DISC_LEARNING_RATE, beta_1=config.BETA1)

    best_ssim = 0.0
    best_spot_acc = 0.0

    for epoch in range(1, config.EPOCHS + 1):
        start_time = time.time()

        if epoch == 40:
            gen_optimizer.learning_rate.assign(config.LEARNING_RATE * 0.5)
            disc_optimizer.learning_rate.assign(config.DISC_LEARNING_RATE * 0.5)
        elif epoch == 70:
            gen_optimizer.learning_rate.assign(config.LEARNING_RATE * 0.25)
            disc_optimizer.learning_rate.assign(config.DISC_LEARNING_RATE * 0.25)

        metrics = {k: tf.keras.metrics.Mean() for k in
                  ['gen_loss', 'disc_loss', 'ssim', 'spot_accuracy', 'l1', 'bright_weighted', 'bright_focal']}

        progress = tqdm(train_dataset, desc=f"Epoch {epoch}/{config.EPOCHS}")
        for inputs, targets in progress:
            losses = train_step(generator, discriminator, gen_optimizer,
                              disc_optimizer, inputs, targets, epoch)

            for k, v in losses.items():
                if k in metrics:
                    metrics[k].update_state(v)

            progress.set_postfix({
                'SSIM': f"{losses['ssim']:.3f}",
                'Spot': f"{losses['spot_accuracy']:.3f}",
                'Gen': f"{losses['gen_loss']:.1f}"
            })

        val_ssim = tf.keras.metrics.Mean()
        val_spot_acc = tf.keras.metrics.Mean()

        for val_inputs, val_targets in val_dataset:
            val_preds = generator(val_inputs, training=False)
            val_ssim.update_state(
                tf.image.ssim((val_targets + 1.0) / 2.0, (val_preds + 1.0) / 2.0, max_val=1.0)
            )
            val_spot_acc.update_state(calculate_spot_accuracy(val_targets, val_preds))

        current_phase = "Structure" if epoch <= 30 else "Bright Spots" if epoch <= 60 else "Refinement"
        print(f"\nEpoch {epoch} [{current_phase}]:")
        print(f"  Time: {time.time() - start_time:.1f}s")
        print(f"  Gen Loss: {metrics['gen_loss'].result():.2f} | Disc Loss: {metrics['disc_loss'].result():.3f}")
        print(f"  L1: {metrics['l1'].result():.3f} | Bright Weighted: {metrics['bright_weighted'].result():.3f} | Bright Focal: {metrics['bright_focal'].result():.3f}")
        print(f"  Train SSIM: {metrics['ssim'].result():.4f} | Val SSIM: {val_ssim.result():.4f}")
        print(f"  Train Spot: {metrics['spot_accuracy'].result():.4f} | Val Spot: {val_spot_acc.result():.4f}")

        if val_ssim.result() > best_ssim:
            best_ssim = val_ssim.result()
            generator.save(os.path.join(config.OUTPUT_DIR, 'best_ssim.keras'))
            print(f"  ✓ New best SSIM: {best_ssim:.4f}")

        if val_spot_acc.result() > best_spot_acc and epoch > 30:
            best_spot_acc = val_spot_acc.result()
            generator.save(os.path.join(config.OUTPUT_DIR, 'best_spot.keras'))
            print(f"  ✓ New best spot accuracy: {best_spot_acc:.4f}")

        if epoch % config.SAMPLE_INTERVAL == 0:
            sample_ssim, sample_spot = generate_samples(
                generator, test_input, test_target, epoch, config.SAMPLE_DIR
            )
            print(f"  Sample SSIM: {sample_ssim:.4f} | Sample Spot: {sample_spot:.4f}")

        if epoch % config.CHECKPOINT_INTERVAL == 0:
            generator.save_weights(f"{config.CHECKPOINT_DIR}/gen_epoch_{epoch}.weights.h5")
            discriminator.save_weights(f"{config.CHECKPOINT_DIR}/disc_epoch_{epoch}.weights.h5")
            print(f"  ✓ Checkpoint saved")

    generator.save(os.path.join(config.OUTPUT_DIR, 'final_generator.keras'))

    print(f"\n" + "=" * 80)
    print(f"Training complete!")
    print(f"Best SSIM: {best_ssim:.4f}")
    print(f"Best Spot Accuracy: {best_spot_acc:.4f}")
    print("=" * 80)

if __name__ == "__main__":
    train_balanced_model()