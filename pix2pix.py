# -*- coding: utf-8 -*-
"""Pix2Pix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNvxvOSpv2Bk58rb-n88YwkRXkqvttzq
"""

import os
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import cv2
import time
from tqdm import tqdm
from PIL import Image, ImageFile
import tifffile
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

Image.MAX_IMAGE_PIXELS = None
ImageFile.LOAD_TRUNCATED_IMAGES = True

print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.config.list_physical_devices('GPU'))

# ==============================================================================
#  CONFIG
# ==============================================================================
class SimplifiedConfig:
    # Data paths
    DATA_DIR = "/kaggle/working/"
    IF_TARGET_FILENAME = "CD8_CD163_PD1_AF_scaled_corrected.tif"
    OUTPUT_DIR = "./simplified_cell_output"
    CHECKPOINT_DIR = "./simplified_cell_checkpoints"
    SAMPLE_DIR = "./simplified_cell_samples"

    # Training parameters
    EPOCHS = 90
    BATCH_SIZE = 4
    IMG_HEIGHT = 512
    IMG_WIDTH = 512
    PATCH_SIZE = 512
    STRIDE = 256


    LAMBDA_L1 = 12.0
    LAMBDA_SSIM = 25.0
    LAMBDA_GRADIENT = 8.0
    LAMBDA_CHANNEL_CONSISTENCY = 12.0
    LAMBDA_FEATURE_MATCH = 6.0
    LAMBDA_GAN = 2.0

    # Learning rates
    LEARNING_RATE = 1e-4
    DISC_LEARNING_RATE = 1e-4
    BETA1 = 0.5
    BETA2 = 0.999

    # Quality targets
    TARGET_SSIM = 0.78
    RED_DOMINANCE_THRESHOLD = 0.55

    # Training phases
    WARMUP_EPOCHS = 5
    FULL_TRAINING_EPOCHS = 15

    # Checkpointing
    CHECKPOINT_INTERVAL = 10
    SAMPLE_INTERVAL = 3
    BACKGROUND_THRESHOLD = 15.0

config = SimplifiedConfig()

# Create directories
for directory in [config.OUTPUT_DIR, config.CHECKPOINT_DIR, config.SAMPLE_DIR]:
    os.makedirs(directory, exist_ok=True)

# GPU Configuration
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"{len(gpus)} Physical GPUs configured")
    except RuntimeError as e:
        print(f"GPU configuration error: {e}")

# ==============================================================================
# SIMPLE BUT EFFECTIVE ATTENTION
# ==============================================================================
class SimpleAttention(layers.Layer):
    """Simple attention without dimension issues"""

    def __init__(self, filters, **kwargs):
        super().__init__(**kwargs)
        self.filters = filters

    def build(self, input_shape):
        self.attention_conv = layers.Conv2D(1, 3, padding='same', activation='sigmoid')
        self.refine_conv = layers.Conv2D(self.filters, 1, padding='same')
        super().build(input_shape)

    def call(self, inputs):
        attention = self.attention_conv(inputs)
        attended = inputs * (1.0 + attention * 0.5)
        refined = self.refine_conv(attended)
        return refined + inputs

# ==============================================================================
# LOSS FUNCTIONS
# ==============================================================================
def simple_ssim_loss(y_true, y_pred):
    """Simple but effective SSIM"""
    y_true_01 = (y_true + 1.0) / 2.0
    y_pred_01 = (y_pred + 1.0) / 2.0

    ssim_values = []
    for c in range(3):
        true_ch = y_true_01[:, :, :, c:c+1]
        pred_ch = y_pred_01[:, :, :, c:c+1]
        ssim_ch = tf.image.ssim(true_ch, pred_ch, max_val=1.0)
        ssim_values.append(tf.reduce_mean(ssim_ch))

    avg_ssim = tf.reduce_mean(ssim_values)
    return 1.0 - avg_ssim

def simple_gradient_loss(y_true, y_pred):
    """Simple gradient loss for edges"""
    def gradient_magnitude(images):
        # Simple gradient using differences
        grad_x = images[:, :, :-1, :] - images[:, :, 1:, :]
        grad_y = images[:, :-1, :, :] - images[:, 1:, :, :]

        # Pad to maintain size
        grad_x = tf.pad(grad_x, [[0, 0], [0, 0], [0, 1], [0, 0]])
        grad_y = tf.pad(grad_y, [[0, 0], [0, 1], [0, 0], [0, 0]])

        return tf.sqrt(grad_x**2 + grad_y**2 + 1e-8)

    true_grad = gradient_magnitude(y_true)
    pred_grad = gradient_magnitude(y_pred)

    return tf.reduce_mean(tf.abs(true_grad - pred_grad))

def simple_channel_consistency_loss(y_true, y_pred):
    """Simple channel consistency"""
    total_loss = 0

    # Individual channel losses
    for c in range(3):
        true_ch = y_true[:, :, :, c]
        pred_ch = y_pred[:, :, :, c]
        ch_loss = tf.reduce_mean(tf.abs(true_ch - pred_ch))
        total_loss += ch_loss

    return total_loss / 3.0

# ==============================================================================
# DATA LOADING
# ==============================================================================
def load_tiff_image_correctly(path):
    try:
        with tifffile.TiffFile(path) as tif:
            image_data = tif.series[0].asarray()
        if image_data.ndim == 3 and image_data.shape[0] < image_data.shape[1] and image_data.shape[0] < image_data.shape[2]:
            image_data = image_data.transpose((1, 2, 0))
        if image_data.shape[-1] > 3:
            image_data = image_data[:, :, :3]
        return image_data
    except Exception as e:
        print(f"Error loading {os.path.basename(path)}: {e}")
        return None

def load_image_pair(pair_dir):
    source_path = os.path.join(pair_dir, "warped_source.tiff")
    target_path = os.path.join(pair_dir, config.IF_TARGET_FILENAME)
    if not os.path.exists(source_path) or not os.path.exists(target_path):
        return None, None
    try:
        source = np.array(Image.open(source_path).convert("RGB"))
    except Exception as e:
        print(f"Error loading source image: {e}")
        return None, None
    target = load_tiff_image_correctly(target_path)
    if source is None or target is None:
        return None, None
    if target.ndim != 3 or target.shape[2] != 3:
        return None, None
    if source.shape[:2] != target.shape[:2]:
        target = cv2.resize(target, (source.shape[1], source.shape[0]), interpolation=cv2.INTER_NEAREST)
    return source, target

def create_aligned_patches(source, target, patch_size, stride, background_threshold):
    h, w = source.shape[:2]
    source_patches, target_patches = [], []
    for y in range(0, h - patch_size + 1, stride):
        for x in range(0, w - patch_size + 1, stride):
            source_patch = source[y:y + patch_size, x:x + patch_size]
            target_patch = target[y:y + patch_size, x:x + patch_size]
            if np.mean(target_patch) > background_threshold:
                if source_patch.shape[:2] == (patch_size, patch_size):
                    source_patches.append(source_patch)
                    target_patches.append(target_patch)
    return source_patches, target_patches

def load_all_data():
    print("Loading simplified cell translation data...")
    all_source_patches, all_target_patches = [], []
    pair_dirs = [os.path.join(config.DATA_DIR, f"Pair{i}") for i in range(1, 21)]
    for pair_dir in tqdm(pair_dirs, desc="Loading pairs"):
        if not os.path.exists(pair_dir):
            continue
        source, target = load_image_pair(pair_dir)
        if source is not None and target is not None:
            source_p, target_p = create_aligned_patches(
                source, target, config.PATCH_SIZE, config.STRIDE,
                background_threshold=config.BACKGROUND_THRESHOLD
            )
            all_source_patches.extend(source_p)
            all_target_patches.extend(target_p)
    print(f"Total patches loaded: {len(all_source_patches)}")
    return np.array(all_source_patches), np.array(all_target_patches)

# ==============================================================================
# PREPROCESSING
# ==============================================================================
MAX_LOG_VALUE = np.log1p(65535)

def normalize_source(image):
    return (tf.cast(image, tf.float32) / 127.5) - 1.0

def normalize_target(image):
    image = tf.cast(image, tf.float32)
    image = tf.math.log1p(image)
    return (image / (MAX_LOG_VALUE / 2.0)) - 1.0

def simple_augmentation(source, target):
    """Simple but effective augmentation"""
    if tf.random.uniform(()) > 0.5:
        source, target = tf.image.flip_left_right(source), tf.image.flip_left_right(target)
    if tf.random.uniform(()) > 0.5:
        source, target = tf.image.flip_up_down(source), tf.image.flip_up_down(target)

    k = tf.random.uniform([], 0, 4, dtype=tf.int32)
    source, target = tf.image.rot90(source, k), tf.image.rot90(target, k)

    return source, target

def preprocess_train(source, target):
    source = normalize_source(source)
    target = normalize_target(target)
    return simple_augmentation(source, target)

def preprocess_val(source, target):
    return normalize_source(source), normalize_target(target)

# ==============================================================================
# SIMPLIFIED GENERATOR - BACK TO BASICS
# ==============================================================================
def simple_conv_block(x, filters, size=3, strides=1, use_attention=False):
    """Simple conv block"""
    x = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    if use_attention:
        x = SimpleAttention(filters)(x)

    return x

def simple_residual_block(x, filters):
    """Simple residual block"""
    shortcut = x
    x = simple_conv_block(x, filters, 3)
    x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)
    return x

def simplified_generator():
    """Simplified generator based on proven architecture"""
    inputs = layers.Input(shape=[config.IMG_HEIGHT, config.IMG_WIDTH, 3])

    # ENCODER
    # Level 1: Basic features (512x512)
    e1 = simple_conv_block(inputs, 64, 7)
    e1 = simple_conv_block(e1, 64, 3)

    # Level 2: Cell features (256x256)
    e2 = simple_conv_block(e1, 128, 3, strides=2)
    e2 = simple_conv_block(e2, 128, 3, use_attention=True)

    # Level 3: Tissue features (128x128)
    e3 = simple_conv_block(e2, 256, 3, strides=2)
    e3 = simple_conv_block(e3, 256, 3, use_attention=True)

    # Level 4: Context (64x64)
    e4 = simple_conv_block(e3, 512, 3, strides=2)
    e4 = simple_conv_block(e4, 512, 3)

    # BOTTLENECK - Simple but effective
    bottleneck = e4
    for _ in range(4):
        bottleneck = simple_residual_block(bottleneck, 512)

    bottleneck = SimpleAttention(512)(bottleneck)

    # DECODER
    # Level 4 to 3: (64x64 -> 128x128)
    d3 = layers.Conv2DTranspose(256, 3, strides=2, padding='same')(bottleneck)
    d3 = layers.BatchNormalization()(d3)
    d3 = layers.ReLU()(d3)
    d3 = layers.Concatenate()([d3, e3])
    d3 = simple_conv_block(d3, 256, 3)

    # Level 3 to 2: (128x128 -> 256x256)
    d2 = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(d3)
    d2 = layers.BatchNormalization()(d2)
    d2 = layers.ReLU()(d2)
    d2 = layers.Concatenate()([d2, e2])
    d2 = simple_conv_block(d2, 128, 3, use_attention=True)

    # Level 2 to 1: (256x256 -> 512x512)
    d1 = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(d2)
    d1 = layers.BatchNormalization()(d1)
    d1 = layers.ReLU()(d1)
    d1 = layers.Concatenate()([d1, e1])
    d1 = simple_conv_block(d1, 64, 3)

    # MULTI-HEAD OUTPUT - Simplified
    shared_features = simple_conv_block(d1, 32, 3)

    # Three specialized heads
    cd8_features = simple_conv_block(shared_features, 16, 3)
    cd8_output = layers.Conv2D(1, 3, padding='same', activation='tanh')(cd8_features)

    cd163_features = simple_conv_block(shared_features, 16, 3)
    cd163_output = layers.Conv2D(1, 3, padding='same', activation='tanh')(cd163_features)

    pd1_features = simple_conv_block(shared_features, 16, 3)
    pd1_output = layers.Conv2D(1, 3, padding='same', activation='tanh')(pd1_features)

    # Simple combination weights
    combination_features = layers.Concatenate()([cd8_features, cd163_features, pd1_features])
    combination_weights = layers.Conv2D(3, 1, padding='same', activation='softmax')(combination_features)

    # Apply weights using Keras operations
    cd8_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1))(combination_weights)
    cd163_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,1], -1))(combination_weights)
    pd1_weight = layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,2], -1))(combination_weights)

    # Weighted outputs
    cd8_weighted = layers.Multiply()([cd8_output, cd8_weight])
    cd163_weighted = layers.Multiply()([cd163_output, cd163_weight])
    pd1_weighted = layers.Multiply()([pd1_output, pd1_weight])

    # Final output
    outputs = layers.Concatenate(axis=-1)([cd8_weighted, cd163_weighted, pd1_weighted])

    # Final refinement
    outputs = simple_conv_block(outputs, 16, 3)
    outputs = layers.Conv2D(3, 1, padding='same', activation='tanh')(outputs)

    return keras.Model(inputs=inputs, outputs=outputs, name='simplified_generator')

# ==============================================================================
# DISCRIMINATOR
# ==============================================================================
def simplified_discriminator():
    """Simple discriminator"""
    input_img = layers.Input(shape=[None, None, 3])
    target_img = layers.Input(shape=[None, None, 3])

    x = layers.Concatenate()([input_img, target_img])

    features = []

    f1 = layers.Conv2D(64, 4, strides=2, padding='same')(x)
    f1 = layers.LeakyReLU(0.2)(f1)
    features.append(f1)

    f2 = layers.Conv2D(128, 4, strides=2, padding='same')(f1)
    f2 = layers.BatchNormalization()(f2)
    f2 = layers.LeakyReLU(0.2)(f2)
    features.append(f2)

    f3 = layers.Conv2D(256, 4, strides=2, padding='same')(f2)
    f3 = layers.BatchNormalization()(f3)
    f3 = layers.LeakyReLU(0.2)(f3)
    features.append(f3)

    f4 = layers.Conv2D(512, 4, strides=1, padding='same')(f3)
    f4 = layers.BatchNormalization()(f4)
    f4 = layers.LeakyReLU(0.2)(f4)
    features.append(f4)

    final_out = layers.Conv2D(1, 4, padding='same')(f4)

    return keras.Model(inputs=[input_img, target_img],
                      outputs=[final_out] + features,
                      name='simplified_discriminator')

# ==============================================================================
# DISCRIMINATOR LOSSES
# ==============================================================================
def discriminator_loss(real_outputs, fake_outputs):
    total_loss = 0
    for real_out, fake_out in zip(real_outputs, fake_outputs):
        real_loss = tf.reduce_mean(tf.square(real_out - 1.0))
        fake_loss = tf.reduce_mean(tf.square(fake_out))
        total_loss += (real_loss + fake_loss) * 0.5
    return total_loss

def generator_gan_loss(fake_outputs):
    total_loss = 0
    for fake_out in fake_outputs:
        total_loss += tf.reduce_mean(tf.square(fake_out - 1.0))
    return total_loss

def feature_matching_loss(real_features, fake_features):
    total_loss = 0
    for real_feat, fake_feat in zip(real_features, fake_features):
        total_loss += tf.reduce_mean(tf.abs(real_feat - fake_feat))
    return total_loss / len(real_features)

# ==============================================================================
# LOSS WEIGHTING
# ==============================================================================
def get_simple_weights(epoch):
    """Simple progressive weighting"""
    if epoch <= config.WARMUP_EPOCHS:
        return {
            'l1': config.LAMBDA_L1,
            'ssim': config.LAMBDA_SSIM * 0.5,
            'gradient': config.LAMBDA_GRADIENT * 0.5,
            'channel_consistency': config.LAMBDA_CHANNEL_CONSISTENCY,
            'feature_match': config.LAMBDA_FEATURE_MATCH * 0.5,
            'gan': config.LAMBDA_GAN * 0.5,
        }
    else:
        return {
            'l1': config.LAMBDA_L1,
            'ssim': config.LAMBDA_SSIM,
            'gradient': config.LAMBDA_GRADIENT,
            'channel_consistency': config.LAMBDA_CHANNEL_CONSISTENCY,
            'feature_match': config.LAMBDA_FEATURE_MATCH,
            'gan': config.LAMBDA_GAN,
        }

# ==============================================================================
# TRAINING STEP
# ==============================================================================
@tf.function
def simple_train_step(generator, discriminator, gen_optimizer, disc_optimizer,
                     input_image, real_image, epoch):
    """Simplified training step"""

    with tf.GradientTape(persistent=True) as tape:
        fake_image = generator(input_image, training=True)

        real_image = tf.cast(real_image, tf.float32)
        fake_image = tf.cast(fake_image, tf.float32)

        # Discriminator predictions
        disc_real_out = discriminator([input_image, real_image], training=True)
        disc_fake_out = discriminator([input_image, fake_image], training=True)

        real_disc_output = disc_real_out[0]
        fake_disc_output = disc_fake_out[0]
        real_features = disc_real_out[1:]
        fake_features = disc_fake_out[1:]

        # Discriminator loss
        disc_loss = discriminator_loss([real_disc_output], [fake_disc_output])

        # Get weights
        weights = get_simple_weights(epoch)

        # Simplified generator losses - only 6 components
        mae = tf.keras.losses.MeanAbsoluteError()

        l1_loss_val = mae(real_image, fake_image) * weights['l1']
        ssim_loss_val = simple_ssim_loss(real_image, fake_image) * weights['ssim']
        gradient_loss_val = simple_gradient_loss(real_image, fake_image) * weights['gradient']
        channel_consistency_val = simple_channel_consistency_loss(real_image, fake_image) * weights['channel_consistency']
        feature_match_val = feature_matching_loss(real_features, fake_features) * weights['feature_match']
        gan_loss_val = generator_gan_loss([fake_disc_output]) * weights['gan']

        total_gen_loss = (l1_loss_val + ssim_loss_val + gradient_loss_val +
                         channel_consistency_val + feature_match_val + gan_loss_val)

    # Gradient updates
    current_disc_loss = tf.reduce_mean(disc_loss)

    # Generator update
    gen_grads = tape.gradient(total_gen_loss, generator.trainable_variables)
    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 1.0)
    gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))

    # Discriminator update
    if current_disc_loss > 0.1:
        disc_grads = tape.gradient(disc_loss, discriminator.trainable_variables)
        disc_grads, _ = tf.clip_by_global_norm(disc_grads, 1.0)
        disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))
        disc_updated = True
    else:
        disc_updated = False

    del tape

    # Quality metrics
    y_true_01 = (real_image + 1.0) / 2.0
    y_pred_01 = (fake_image + 1.0) / 2.0
    current_ssim = tf.reduce_mean(tf.image.ssim(y_true_01, y_pred_01, max_val=1.0))

    # Channel balance
    channel_means = tf.reduce_mean(y_pred_01, axis=[1, 2])
    total_intensity = tf.reduce_sum(channel_means, axis=1, keepdims=True) + 1e-8
    channel_ratios = channel_means / total_intensity
    red_ratio = tf.reduce_mean(channel_ratios[:, 0])

    return {
        'total_gen_loss': total_gen_loss,
        'disc_loss': disc_loss,
        'l1_loss': l1_loss_val / weights['l1'],
        'ssim': current_ssim,
        'ssim_loss': ssim_loss_val / weights['ssim'],
        'gradient_loss': gradient_loss_val / weights['gradient'],
        'channel_consistency': channel_consistency_val / weights['channel_consistency'],
        'red_ratio': red_ratio,
        'disc_updated': disc_updated
    }

# ==============================================================================
# SAMPLE GENERATION
# ==============================================================================
def generate_simple_samples(model, test_inputs, test_targets, epoch, save_dir):
    """Simple sample generation"""
    if test_inputs is None:
        return None, None

    num_samples = min(3, test_inputs.shape[0])
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))

    if num_samples == 1:
        axes = axes.reshape(1, -1)

    fig.suptitle(f'Simplified Cell Translation - Epoch {epoch}', fontsize=16)

    ssim_scores = []
    red_ratios = []

    for i in range(num_samples):
        input_image = tf.expand_dims(test_inputs[i], 0)
        target_image = tf.expand_dims(test_targets[i], 0)

        prediction = model(input_image, training=False)[0]

        # Quality metrics
        target_f32 = tf.cast(target_image[0], tf.float32)
        pred_f32 = tf.cast(prediction, tf.float32)

        y_true_01 = (target_f32 + 1.0) / 2.0
        y_pred_01 = (pred_f32 + 1.0) / 2.0

        # SSIM
        sample_ssim = tf.image.ssim(tf.expand_dims(y_true_01, 0),
                                    tf.expand_dims(y_pred_01, 0), max_val=1.0).numpy()[0]
        ssim_scores.append(sample_ssim)

        # Channel balance
        ch_means = tf.reduce_mean(y_pred_01, axis=[0, 1])
        total_int = tf.reduce_sum(ch_means) + 1e-8
        red_ratio = (ch_means[0] / total_int).numpy()
        red_ratios.append(red_ratio)

        # Prepare images
        source_img = (input_image[0] * 0.5 + 0.5).numpy()

        target_log = (target_image[0] + 1.0) * (MAX_LOG_VALUE / 2.0)
        pred_log = (prediction + 1.0) * (MAX_LOG_VALUE / 2.0)

        target_linear = tf.math.expm1(target_log).numpy()
        pred_linear = tf.math.expm1(pred_log).numpy()

        # Enhanced visualization
        target_enhanced = np.zeros_like(target_linear)
        pred_enhanced = np.zeros_like(pred_linear)

        for c in range(3):
            tg_chan = target_linear[:, :, c]
            if tg_chan.max() > tg_chan.min():
                p1, p99 = np.percentile(tg_chan, (1, 99))
                target_enhanced[:, :, c] = np.clip((tg_chan - p1) / (p99 - p1 + 1e-9), 0, 1)

            pr_chan = pred_linear[:, :, c]
            if pr_chan.max() > pr_chan.min():
                p1, p99 = np.percentile(pr_chan, (1, 99))
                pred_enhanced[:, :, c] = np.clip((pr_chan - p1) / (p99 - p1 + 1e-9), 0, 1)

        # Display
        axes[i, 0].imshow(source_img)
        axes[i, 0].set_title("Input (H&E)")
        axes[i, 0].axis('off')

        axes[i, 1].imshow(target_enhanced)
        axes[i, 1].set_title("Ground Truth (IF)")
        axes[i, 1].axis('off')

        balance_status = "Balanced" if red_ratio <= config.RED_DOMINANCE_THRESHOLD else "Red Dom"
        pred_title = f"Predicted (IF)\nSSIM: {sample_ssim:.3f} | {balance_status}: {red_ratio:.3f}"
        axes[i, 2].imshow(pred_enhanced)
        axes[i, 2].set_title(pred_title)
        axes[i, 2].axis('off')

    # Overall assessment
    avg_ssim = np.mean(ssim_scores)
    avg_red_ratio = np.mean(red_ratios)
    balanced_count = sum([r <= config.RED_DOMINANCE_THRESHOLD for r in red_ratios])

    if avg_ssim > config.TARGET_SSIM and balanced_count == num_samples:
        status = "TARGET ACHIEVED!"
        color = "green"
    elif balanced_count == num_samples:
        status = "Well Balanced"
        color = "blue"
    else:
        status = "Learning..."
        color = "orange"

    status_line = f'{status} | SSIM: {avg_ssim:.4f} | Red Ratio: {avg_red_ratio:.3f}'
    fig.text(0.5, 0.02, status_line, ha='center', fontsize=12, color=color)

    plt.tight_layout(rect=[0, 0.04, 1, 0.96])
    plt.savefig(os.path.join(save_dir, f'simple_epoch_{epoch:04d}.png'), dpi=150, bbox_inches='tight')
    plt.close()

    return avg_ssim, avg_red_ratio

# ==============================================================================
# MAIN TRAINING FUNCTION
# ==============================================================================
def train_simplified_model():
    print("=" * 80)
    print("SIMPLIFIED CELL TRANSLATION - BACK TO BASICS")
    print("Proven approach with essential components only")
    print("=" * 80)

    # Load data
    print("\nLoading data...")
    source_patches, target_patches = load_all_data()

    # Split data
    train_source, val_source, train_target, val_target = train_test_split(
        source_patches, target_patches, test_size=0.1, random_state=42
    )
    print(f"Train samples: {len(train_source)}, Validation samples: {len(val_source)}")

    # Create datasets
    with tf.device('/CPU:0'):
        train_dataset = tf.data.Dataset.from_tensor_slices((train_source, train_target))
        val_dataset = tf.data.Dataset.from_tensor_slices((val_source, val_target))

    AUTOTUNE = tf.data.AUTOTUNE
    train_dataset = train_dataset.map(preprocess_train, num_parallel_calls=AUTOTUNE)
    train_dataset = train_dataset.shuffle(1000).batch(config.BATCH_SIZE).prefetch(AUTOTUNE)

    val_dataset = val_dataset.map(preprocess_val, num_parallel_calls=AUTOTUNE)
    val_dataset = val_dataset.batch(config.BATCH_SIZE).prefetch(AUTOTUNE)

    print("Simplified datasets created.")

    # Build simplified architecture
    print("\nBuilding Simplified architecture...")
    generator = simplified_generator()
    discriminator = simplified_discriminator()

    print(f"Generator parameters: {generator.count_params():,}")
    print(f"Discriminator parameters: {discriminator.count_params():,}")

    # Setup optimizers
    gen_optimizer = tf.optimizers.Adam(config.LEARNING_RATE, beta_1=config.BETA1, beta_2=config.BETA2)
    disc_optimizer = tf.optimizers.Adam(config.DISC_LEARNING_RATE, beta_1=config.BETA1, beta_2=config.BETA2)

    # Prepare validation samples
    if len(val_source) > 0:
        indices = np.random.choice(len(val_source), min(3, len(val_source)), replace=False)
        sample_source, sample_target = val_source[indices], val_target[indices]

        test_input = tf.stack([preprocess_val(s, t)[0] for s, t in zip(sample_source, sample_target)])
        test_target = tf.stack([preprocess_val(s, t)[1] for s, t in zip(sample_source, sample_target)])
    else:
        test_input, test_target = None, None

    print(f"\nStarting Simplified training...")
    print("Key simplifications:")
    print("  - Only 6 loss components (vs 8)")
    print("  - Simplified architecture")
    print("  - Reduced loss weights")
    print("  - Focus on proven techniques")
    print("  - Target gen loss ~30 (like original)")
    print("=" * 80)

    # Training tracking
    best_ssim = 0.0
    best_val_loss = float('inf')
    target_achieved = False

    for epoch in range(1, config.EPOCHS + 1):
        start_time = time.time()

        # Training metrics
        metrics = {
            'gen_loss': tf.keras.metrics.Mean(),
            'disc_loss': tf.keras.metrics.Mean(),
            'l1_loss': tf.keras.metrics.Mean(),
            'ssim': tf.keras.metrics.Mean(),
            'gradient_loss': tf.keras.metrics.Mean(),
            'red_ratio': tf.keras.metrics.Mean()
        }

        # Training loop
        steps_per_epoch = len(train_source) // config.BATCH_SIZE
        progress_bar = tqdm(train_dataset, desc=f"Epoch {epoch}/{config.EPOCHS}",
                          total=steps_per_epoch, leave=True)

        for input_image, real_image in progress_bar:
            losses = simple_train_step(generator, discriminator, gen_optimizer,
                                     disc_optimizer, input_image, real_image, epoch)

            # Update metrics
            for key in metrics:
                if key in losses:
                    metrics[key].update_state(losses[key])

            # Progress display with disc loss
            progress_bar.set_postfix({
                "SSIM": f"{losses['ssim']:.3f}",
                "Red%": f"{losses['red_ratio']:.3f}",
                "L1": f"{losses['l1_loss']:.3f}",
                "Gen": f"{losses['total_gen_loss']:.2f}",
                "Disc": f"{losses['disc_loss']:.3f}"
            })

        # Validation
        val_metrics = {
            'l1_loss': tf.keras.metrics.Mean(),
            'ssim': tf.keras.metrics.Mean()
        }

        for val_input, val_target in val_dataset:
            val_pred = generator(val_input, training=False)

            val_target = tf.cast(val_target, tf.float32)
            val_pred = tf.cast(val_pred, tf.float32)

            val_metrics['l1_loss'].update_state(tf.keras.losses.mae(val_target, val_pred))

            val_target_01 = (val_target + 1.0) / 2.0
            val_pred_01 = (val_pred + 1.0) / 2.0
            val_metrics['ssim'].update_state(tf.image.ssim(val_target_01, val_pred_01, max_val=1.0))

        # Current metrics
        current_val_ssim = val_metrics['ssim'].result().numpy()
        current_red_ratio = metrics['red_ratio'].result().numpy()
        current_gen_loss = metrics['gen_loss'].result().numpy()
        current_disc_loss = metrics['disc_loss'].result().numpy()

        # Epoch summary
        print(f"\nEpoch {epoch}:")
        print(f"  Time: {time.time() - start_time:.2f}s")
        print(f"  Gen Loss: {current_gen_loss:.2f} | Disc Loss: {current_disc_loss:.3f}")
        print(f"  Train SSIM: {metrics['ssim'].result():.4f} | Val SSIM: {current_val_ssim:.4f}")
        print(f"  Red Ratio: {current_red_ratio:.3f}")

        # Target achievement
        if current_val_ssim > config.TARGET_SSIM and current_red_ratio <= config.RED_DOMINANCE_THRESHOLD and not target_achieved:
            target_achieved = True
            print(f"  TARGET ACHIEVED! SSIM = {current_val_ssim:.4f} > {config.TARGET_SSIM}")
            generator.save(os.path.join(config.OUTPUT_DIR, 'target_achieved_simplified.keras'))

        # Save best models
        if current_val_ssim > best_ssim:
            best_ssim = current_val_ssim
            generator.save(os.path.join(config.OUTPUT_DIR, 'best_simplified_ssim.keras'))
            print(f"  New best SSIM: {best_ssim:.4f}")

        if val_metrics['l1_loss'].result() < best_val_loss:
            best_val_loss = val_metrics['l1_loss'].result()
            generator.save(os.path.join(config.OUTPUT_DIR, 'best_simplified_l1.keras'))

        # Generate samples
        if epoch % config.SAMPLE_INTERVAL == 0 and test_input is not None:
            sample_ssim, sample_red_ratio = generate_simple_samples(
                generator, test_input, test_target, epoch, config.SAMPLE_DIR)
            if sample_ssim:
                print(f"  Sample SSIM: {sample_ssim:.4f}, Red: {sample_red_ratio:.3f}")

        # Save checkpoints
        if epoch % config.CHECKPOINT_INTERVAL == 0:
            checkpoint_prefix = os.path.join(config.CHECKPOINT_DIR, f"simplified_epoch_{epoch}")
            generator.save_weights(f"{checkpoint_prefix}_generator.weights.h5")
            discriminator.save_weights(f"{checkpoint_prefix}_discriminator.weights.h5")

        # Early stopping
        if target_achieved and current_gen_loss < 35:
            print(f"\nTarget achieved with stable low loss!")
            break

    # Save final model
    final_model_path = os.path.join(config.OUTPUT_DIR, 'final_simplified_generator.keras')
    generator.save(final_model_path)

    print(f"\nSimplified training completed!")
    print(f"Best SSIM: {best_ssim:.4f}")
    print(f"Target achieved: {target_achieved}")
    print(f"Final model: {final_model_path}")

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
if __name__ == "__main__":
    try:
        train_simplified_model()
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()